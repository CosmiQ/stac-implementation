{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MVS Dataset hosted on SpaceNet to be Stac Compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import shapely\n",
    "import sys,os,os.path\n",
    "\n",
    "# Rasterio python expect ssl certs in Centos location\n",
    "os.environ['CURL_CA_BUNDLE']='/etc/ssl/certs/ca-certificates.crt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stac_tools.stac_item import spacenetStacItem\n",
    "\n",
    "\n",
    "imtypeDict = {'P1BS': \"Pan Band 1B\",\n",
    "              'A1BS': \"SWIR Band 1B\",\n",
    "              \"M1BS\": \"Multi-Spectral 1B\"\n",
    "             }\n",
    "imextDict = {'.tar': \"Meta Data Archive\",\n",
    "             '.NTF': 'NITF',\n",
    "             '.rm' : \"RM metadata file\",\n",
    "             '.tif': \"COG\",\n",
    "             '.vrt': \"vrt\"\n",
    "            }\n",
    "\n",
    "def write_assetDict(df, imextDict=imextDict, imtypeDict=imtypeDict):\n",
    "    \n",
    "    assetDict = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['ext'] == '.vrt':\n",
    "            pass\n",
    "        else:\n",
    "            assetDict.update({row['basename']: {\n",
    "                                            \"href\": row['s3loc'],\n",
    "                                            \"type\": row['ext'],\n",
    "                                            \"name\": imtypeDict[row['imtype']] + \" \" + imextDict[row['ext']]\n",
    "                                            }})\n",
    "\n",
    "        \n",
    "    return assetDict\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def write_linkDict(stac_path, catalog_link=\"\", collection_link=\"\"):\n",
    "    linkDict = {\"self\": {\"rel\":\"self\",\n",
    "                        \"href\":stac_path},\n",
    "               }\n",
    "    \n",
    "    if collection_link != \"\":\n",
    "        linkDict.update({\"collection\": {\"rel\": \"collection\",\n",
    "                              \"href\": collection_link}\n",
    "                        }\n",
    "                       )\n",
    "    \n",
    "    if catalog_link != \"\":\n",
    "        linkDict.update({\"catalog\": {\"rel\": \"catalog\",\n",
    "                              \"href\": catalog_link}\n",
    "                        }\n",
    "                       )\n",
    "        \n",
    "    \n",
    "    \n",
    "    return linkDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def writeStac_Item(out_file, assetDF, stac_path, cog_path, thumbnail_path, imd_path=[], vrtPath=[] ,catalog_path='', collection_path='', idStr=[]):\n",
    "    \n",
    "    \n",
    "    linkDict = write_linkDict(stac_path, catalog_link=catalog_path, collection_link=collection_path)\n",
    "    \n",
    "    assetDict = write_assetDict(assetDF)\n",
    "    \n",
    "    if idStr:\n",
    "        pass\n",
    "    else: \n",
    "        idStr = os.path.splitext(os.path.basename(imd_path))[0]\n",
    "    stac_Item = spacenetStacItem(rasterPath=cog_path, \n",
    "                                 provider='DigitalGlobe', \n",
    "                                 license=\"Commercial satellite imagery in the MVS benchmark data set was provided courtesy of DigitalGlobe.\", \n",
    "                                idStr=idStr, \n",
    "                                 assetDict=assetDict, \n",
    "                                 imdPath=imd_path, \n",
    "                                 vrtPath=vrtPath,\n",
    "                                 links=linkDict)\n",
    "    \n",
    "    stac_Item.write_toJSON(out_file)\n",
    "\n",
    "    \n",
    "    return stac_Item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect imagery Summary\n",
    "\n",
    "from os import walk\n",
    "import os\n",
    "f = []\n",
    "ftype = []\n",
    "mypath = '/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/WV3/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    print(os.path.basename(dirpath))\n",
    "    \n",
    "    f.extend([os.path.join(dirpath, file) for file in filenames])\n",
    "    ftype.extend([os.path.basename(dirpath) for file in filenames])\n",
    "\n",
    "basename_list = []\n",
    "ext_list      = []\n",
    "\n",
    "cell_list = {}\n",
    "idstr_list = []\n",
    "imtype_list = []\n",
    "sloc_list = []\n",
    "for file in f:\n",
    "    basename = os.path.basename(file)\n",
    "    basesplit = basename.split('-')\n",
    "    idStr = basesplit[0]\n",
    "    imType = basesplit[1]\n",
    "    \n",
    "    \n",
    "    idstr_list.append(basesplit[0])\n",
    "    imtype_list.append(basesplit[1])\n",
    "    basename_list.append(os.path.basename(file))\n",
    "    ext_list.append(os.path.splitext(file)[1])\n",
    "    sloc_list.append(file.replace('/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/', 's3://spacenet-dataset/mvs_dataset/'))\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "dataList = {'idstr': idstr_list,\n",
    "           \"imtype\": imtype_list,\n",
    "           \"basename\": basename_list,\n",
    "           \"ext\": ext_list,\n",
    "           \"filepath\": f,\n",
    "            \"s3loc\": sloc_list\n",
    "           }\n",
    "df = pd.DataFrame(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localSave = \"/raid/nfs/workingDir/spacenet-stac/mvs-dataset/\"\n",
    "idstr = df['idstr'].unique()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "itemList = []\n",
    "# Create Pan Band Item\n",
    "for idstr in tqdm(df['idstr'].unique()):\n",
    "    print(idstr)\n",
    "    tmpDF = df[df['idstr']==idstr]\n",
    "\n",
    "    try:\n",
    "        imgType = 'P1BS'\n",
    "        assetDF = tmpDF[tmpDF['imtype']==imgType]\n",
    "        #print(assetDF.head())\n",
    "        stac_path = \"{}_{}.json\".format(idstr, imgType)\n",
    "        out_file = os.path.join(\"/raid/nfs/workingDir/dlindenbaum/spacenet-stac/spacenet-stac/mvs-dataset/\", stac_path)\n",
    "        print(out_file)\n",
    "        cog_path = assetDF[assetDF['ext']=='.vrt']['filepath'].values[0]\n",
    "        thumbnail_path = ''\n",
    "        vrt_path = cog_path\n",
    "        #print(cog_path)\n",
    "        catalog_path = \"../mvs-dataset.json\"\n",
    "        collection_path = \"../spacenet-collections/spacenet-WV3-1B.json\"\n",
    "        imd_path = []\n",
    "\n",
    "        writeStac_Item(out_file, assetDF, stac_path, cog_path, thumbnail_path, imd_path, vrt_path, catalog_path, collection_path, idstr)\n",
    "        itemList.append(out_file)\n",
    "    except:\n",
    "        print(\"ERROR: {}\".format(stac_path))\n",
    "\n",
    "\n",
    "    try:\n",
    "        imgType = 'M1BS'\n",
    "        assetDF = tmpDF[tmpDF['imtype']==imgType]\n",
    "        #print(assetDF.head())\n",
    "        stac_path = \"{}_{}.json\".format(idstr, imgType)\n",
    "        out_file = os.path.join(\"/raid/nfs/workingDir/dlindenbaum/spacenet-stac/spacenet-stac/mvs-dataset/\", stac_path)\n",
    "        cog_path = assetDF[assetDF['ext']=='.vrt']['filepath'].values[0]\n",
    "        thumbnail_path = ''\n",
    "        vrt_path = cog_path\n",
    "        #print(cog_path)\n",
    "        catalog_path = \"../mvs-dataset.json\"\n",
    "        collection_path = \"../spacenet-collections/spacenet-WV3-1B.json\"\n",
    "        imd_path = []\n",
    "        writeStac_Item(out_file, assetDF, stac_path, cog_path, thumbnail_path, imd_path, vrt_path, catalog_path, collection_path, idstr)\n",
    "        itemList.append(out_file)\n",
    "    except:\n",
    "        \n",
    "        print(\"ERROR: {}\".format(stac_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Get Pan DataFrame\n",
    "\n",
    "## Read PanVRT\n",
    "\n",
    "## Process EO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idstr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_Dict = {'name': 'Multi-View Stereo Dataset',\n",
    "               \"description\": r\"The availability of public multiple view stereo (MVS) benchmark datasets has been instrumental in enabling research to advance the state of the art in the field and to apply and customize methods to real-world problems. In this work, we provide a public benchmark data set for multiple view stereo applied to 3D outdoor scene mapping using commercial satellite imagery. This data set includes DigitalGlobe WorldView-3 panchromatic and multispectral images of a 100 square kilometer area near San Fernando, Argentina. We also provide 20cm airborne lidar ground truth data for a 20 square kilometer subset of this area and performance analysis software to assess accuracy and completeness metrics. Commercial satellite imagery is provided courtesy of DigitalGlobe, and ground truth lidar is provided courtesy of IARPA.This data supported the IARPA Multi-View Stereo 3D Mapping Challenge and is now made publicly available with no restrictions to support continued research. JHU/APL does not plan to maintain an online benchmark leaderboard, but we welcome your feedback and would love to hear about what youâ€™re doing with the data and include your published results on this page.  SpaceNet is hosting the Multi-View Stereo 3D Mapping dataset in the spacenet repository to ensure easy access to the data.\",\n",
    "               \"license\": {\"name\": \"Commercial satellite imagery in the MVS benchmark data set was provided courtesy of DigitalGlobe.\"},\n",
    "               \"contact\": {\n",
    "        \"name\": \"SpaceNet Team\",\n",
    "        \"email\": \"@dlindenbaum\",\n",
    "        \"url\": \"http://spacenetchallenge.github.io\"\n",
    "    },\n",
    "                \"formats\": [\"geotiff\", \"cog\", \"NITF\"],\n",
    "\n",
    "    \"keywords\": [\"aerial\", \"machine-learning\", \"deep learning\", \"Stereo\", \"LIDAR\"],\n",
    "    \"homepage\": \"https://spacenetchallenge.github.io/datasets/mvs_summary.html\",\n",
    "\n",
    "    \"provider\": {\n",
    "        \"scheme\": \"s3\",\n",
    "        \"region\": \"us-east-1\",\n",
    "        \"requesterPays\": \"false\"\n",
    "    }\n",
    "               \n",
    "               }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkList = []\n",
    "linkList.append({'rel': \"self\",\n",
    "               \"href\": \"mvs-dataset.json\"})\n",
    "for item in itemList:\n",
    "    \n",
    "    linkItem = {'rel': \"item\",\n",
    "               \"href\": \"mvs-dataset/{}\".format(os.path.basename(item))}\n",
    "    \n",
    "    linkList.append(linkItem)\n",
    "    \n",
    "catalog_Dict.update({\"links\": linkList})\n",
    "\n",
    "import json\n",
    "with open(\"../spacenet-stac/mvs-dataset.json\", 'w') as fp:\n",
    "    json.dump(catalog_Dict, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/radiantearth/stac-spec/blob/dev/json-spec/examples/digitalglobe-sample.json\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/radiantearth/stac-spec/blob/dev/json-spec/examples/digitalglobe-sample.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "f = []\n",
    "ftype = []\n",
    "mypath = '/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/WV3/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    print(os.path.basename(dirpath))\n",
    "    \n",
    "    f.extend([os.path.join(dirpath, file) for file in filenames])\n",
    "    ftype.extend([os.path.basename(dirpath) for file in filenames])\n",
    "\n",
    "basename_list = []\n",
    "ext_list      = []\n",
    "\n",
    "cell_list = {}\n",
    "idstr_list = []\n",
    "imtype_list = []\n",
    "sloc_list = []\n",
    "for file in f:\n",
    "    basename = os.path.basename(file)\n",
    "    basesplit = basename.split('-')\n",
    "    idStr = basesplit[0]\n",
    "    imType = basesplit[1]\n",
    "    \n",
    "    \n",
    "    idstr_list.append(basesplit[0])\n",
    "    imtype_list.append(basesplit[1])\n",
    "    basename_list.append(os.path.basename(file))\n",
    "    ext_list.append(os.path.splitext(file)[1])\n",
    "    sloc_list.append(file.replace('/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/', 's3://spacenet-dataset/mvs_dataset/'))\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "dataList = {'idstr': idstr_list,\n",
    "           \"imtype\": imtype_list,\n",
    "           \"basename\": basename_list,\n",
    "           \"ext\": ext_list,\n",
    "           \"filepath\": f,\n",
    "            \"s3loc\": sloc_list\n",
    "           }\n",
    "df = pd.DataFrame(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['imtype'].unique())\n",
    "imtypeDict = {'P1BS': \"Pan Band 1B\",\n",
    "              'A1BS': \"SWIR Band 1B\",\n",
    "              \"M1BS\": \"Multi-Spectral 1B\"\n",
    "             }\n",
    "print(df['ext'].unique())\n",
    "imextDict = {'.tar': \"Meta Data Archive\",\n",
    "             '.NTF': 'NITF',\n",
    "             '.rm' : \"RM metadata file\",\n",
    "             '.tif': \"COG\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idstr in df['idstr'].unique():\n",
    "    tmpDF = df[df['idstr']==idstr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDF = df[df['idstr']==df['idstr'].unique()[0]]\n",
    "print(tmpDF['filepath'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idstr = df['idstr'].unique()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDF = df[df['idstr']==idstr]\n",
    "\n",
    "# Create Pan Band Item\n",
    "imDF = tmpDF[tmpDF['imtype']=='P1BS']\n",
    "vrtFilePath = imDF[imDF['ext']=='.vrt']['filepath'].values[0]\n",
    "print(vrtFilePath)\n",
    "imDF.head()\n",
    "\n",
    "## Get Pan DataFrame\n",
    "\n",
    "## Read PanVRT\n",
    "\n",
    "## Process EO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rasterio.open(vrtFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.meta\n",
    "tags = src.tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags['NITF_USE00A_SUN_AZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rasterio.open(\"/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/WV3/PAN/30JUN15WV031000015JUN30135323-P1BS-500497282080_01_P001_________AAE_0AAAAABPABP0.NTF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtest = src.tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtest.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "cogLoc = \"/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/WV3/COG/\"\n",
    "\n",
    "smallDF = df[df['ext']=='.NTF']\n",
    "\n",
    "for filepath, basename in tqdm(zip(smallDF['filepath'].values, smallDF['basename'].values)):\n",
    "    print(basename)\n",
    "    cmd = ['gdalwarp', '-rpc', '-of', 'VRT', filepath, os.path.join(cogLoc, os.path.splitext(basename)[0]+\".vrt\")]\n",
    "    subprocess.run(cmd, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datettime import datetime, date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\"/raid/nfs/data/Datasets/CosmiQ_General_Study/MVS_Dataset/WV3/COG/01SEP15WV031000015SEP01135603-M1BS-500497284040_01_P001_________GA_E0AAAAAAKAAK0.vrt\") as src:\n",
    "    tags = src.tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags['NITF_STDIDC_ACQUISITION_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time\n",
    "dt = datetime.strptime('20150901135603', \"%Y%m%d%H%M%S\")\n",
    "dt.isoformat('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cw_environment]",
   "language": "python",
   "name": "conda-env-cw_environment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
